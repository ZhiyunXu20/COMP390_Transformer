{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1KQ4DDnreYfzK3cicJtrrbXOSIY5juDsY","timestamp":1729676378396}],"authorship_tag":"ABX9TyPeu/n5K4nvT/mGMFHQZkfb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMV_YJy4GMrE","executionInfo":{"status":"ok","timestamp":1729665633359,"user_tz":-60,"elapsed":23882,"user":{"displayName":"徐执允","userId":"06120118557581498882"}},"outputId":"d1973a50-be88-4679-a8e7-def68177fe9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","import os\n","# 添加路径到系统路径\n","sys.path.append(\"/content/drive/MyDrive/transformer\")\n","# Change working directory to the script's location\n","os.chdir(\"/content/drive/MyDrive/transformer\")\n","\n","# raw data\n","path_do_data = \"data/english.txt\"\n","data_raw = open(path_do_data, encoding=\"utf-8\").read()"],"metadata":{"id":"_UPkK4KyMRFC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -r /content/drive/MyDrive/transformer/requirements.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zdfyfnvMURN","executionInfo":{"status":"ok","timestamp":1729667174106,"user_tz":-60,"elapsed":5847,"user":{"displayName":"徐执允","userId":"06120118557581498882"}},"outputId":"d70828a2-f3aa-4fd1-bc44-e8aadde9a8b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/transformer/requirements.txt (line 2)) (2.4.1+cu121)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (4.44.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/transformer/requirements.txt (line 4)) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 2)) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 2)) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 2)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 2)) (3.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 2)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 2)) (2024.6.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (0.24.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (4.66.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 2)) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 3)) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->-r /content/drive/MyDrive/transformer/requirements.txt (line 2)) (1.3.0)\n"]}]},{"cell_type":"code","source":["!python /content/drive/MyDrive/transformer/train_model.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcwMsK_lMra4","executionInfo":{"status":"ok","timestamp":1729669362000,"user_tz":-60,"elapsed":1294696,"user":{"displayName":"徐执允","userId":"06120118557581498882"}},"outputId":"cf7266a3-bb42-4ec4-dafc-d70eb7a1d777"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Token indices sequence length is longer than the specified maximum sequence length for this model (1239778 > 512). Running this sequence through the model will result in indexing errors\n","Model with 89.48M parameters\n","Training Progress:   0% 0/5000 [00:00<?, ?it/s]step          0 | train loss 10.6753 | val loss 10.6958\n","Training Progress:   1% 63/5000 [21:19<27:50:43, 20.30s/it]\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/transformer/train_model.py\", line 80, in <module>\n","    logits, loss = m.forward(xb, yb)\n","  File \"/content/drive/MyDrive/transformer/model.py\", line 169, in forward\n","    x = self.blocks(x)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 219, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/transformer/model.py\", line 124, in forward\n","    x = x + self.ffwd(self.ln2(x))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/transformer/model.py\", line 94, in forward\n","    return self.net(x)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 219, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 117, in forward\n","    return F.linear(input, self.weight, self.bias)\n","KeyboardInterrupt\n"]}]},{"cell_type":"code","source":["# 加载并使用训练好的模型进行推理\n","import torch\n","from transformers import AutoTokenizer\n","from model import Transformer\n","from utils import load_model_from_checkpoint, decode"],"metadata":{"id":"OMlC6c_cVztv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 设置参数\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","path_to_checkpoint = \"/content/drive/MyDrive/transformer/checkpoints/state_dict_model.pt\"\n","vocab_path = \"bert-base-uncased\"\n","\n","# 加载分词器\n","tokenizer = AutoTokenizer.from_pretrained(vocab_path)\n","vocab_size = tokenizer.vocab_size\n","\n","# 加载模型\n","model = Transformer(\n","    vocab_size=vocab_size,\n","    num_embed=768,  # 假设 num_embed=768，与之前训练时一致\n","    block_size=64,\n","    num_heads=6,\n","    num_layers=6,\n","    dropout=0.2,\n",").to(DEVICE)\n","\n","# hyperparameters\n","BATCH_SIZE = 32  # how many independent sequences will we process in parallel?\n","BLOCK_SIZE = 64  # what is the maximum context length for predictions?\n","MAX_ITER = 5000  # number of training iterations\n","EVAL_INTER = 500\n","LEARNING_RATE = 3e-4\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","NUM_HEAD = 6\n","NUM_EMBED = NUM_HEAD * 128\n","NUM_LAYER = 6\n","DROPOUT = 0.2\n","\n","# 从检查点加载权重\n","model = load_model_from_checkpoint(model_class=Transformer, path_to_checkpoint=path_to_checkpoint, vocab_size=vocab_size)"],"metadata":{"id":"IsPX6PGTV9Is"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 生成文本\n","context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n","generated_tokens = model.generate(idx=context, max_new_tokens=100, block_size=64)[0]\n","generated_text = decode(enc_sec=generated_tokens, tokenizer=tokenizer)\n","\n","print(\"Generated text:\\n\", generated_text)"],"metadata":{"id":"oiN9CNwUWGTb"},"execution_count":null,"outputs":[]}]}